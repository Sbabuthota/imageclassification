{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM9W9k+etrUB/B+LWgc1O3O",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sbabuthota/imageclassification/blob/main/Copy_of_preprocess_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjKiPCffx6LC",
        "outputId": "1ae6b3e2-ff41-4f96-95ec-bfca9edf3a11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170498071/170498071 [==============================] - 3s 0us/step\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load the CIFAR-10 dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n",
        "\n",
        "# Resize images to 64x64 (if necessary)\n",
        "train_images_resized = tf.image.resize(train_images, (64, 64))\n",
        "test_images_resized = tf.image.resize(test_images, (64, 64))\n",
        "\n",
        "# Normalize images to the range [0, 1]\n",
        "train_images_normalized = train_images_resized / 255.0\n",
        "test_images_normalized = test_images_resized / 255.0\n",
        "\n",
        "# Convert class vectors to binary class matrices (one-hot encoding)\n",
        "train_labels_one_hot = to_categorical(train_labels, num_classes=10)\n",
        "test_labels_one_hot = to_categorical(test_labels, num_classes=10)\n",
        "\n",
        "# Save preprocessed data locally\n",
        "np.save('train_images_normalized.npy', train_images_normalized)\n",
        "np.save('test_images_normalized.npy', test_images_normalized)\n",
        "np.save('train_labels_one_hot.npy', train_labels_one_hot)\n",
        "np.save('test_labels_one_hot.npy', test_labels_one_hot)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install numpy opencv-python pillow\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jHedFOKQ-UP1",
        "outputId": "3ff36cb9-0f41-497e-bf02-80082161f53e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.8.0.76)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (9.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import cv2  # OpenCV for resizing\n",
        "\n",
        "# Function to resize images\n",
        "def resize_images(images, new_size):\n",
        "    resized_images = []\n",
        "    for img in images:\n",
        "        resized_img = cv2.resize(img, new_size, interpolation=cv2.INTER_AREA)\n",
        "        resized_images.append(resized_img)\n",
        "    return np.array(resized_images)\n",
        "\n",
        "# Function to save images as files (optional)\n",
        "def save_images(images, labels, directory, prefix):\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "    for i, img in enumerate(images):\n",
        "        filename = f\"{prefix}_{i}.png\"\n",
        "        filepath = os.path.join(directory, filename)\n",
        "        cv2.imwrite(filepath, img)\n",
        "        if labels is not None:\n",
        "            label = labels[i]\n",
        "            label_filepath = os.path.join(directory, f\"{prefix}_label_{i}.txt\")\n",
        "            with open(label_filepath, 'w') as f:\n",
        "                f.write(str(label))\n",
        "\n",
        "# Load the .npy files\n",
        "train_images = np.load('train_images_normalized.npy')\n",
        "test_images = np.load('test_images_normalized.npy')\n",
        "train_labels = np.load('train_labels_one_hot.npy')\n",
        "test_labels = np.load('test_labels_one_hot.npy')\n",
        "\n",
        "# Define the new size (width, height)\n",
        "new_size = (32, 32)\n",
        "\n",
        "# Resize the images\n",
        "train_images_resized = resize_images(train_images, new_size)\n",
        "test_images_resized = resize_images(test_images, new_size)\n",
        "\n",
        "# Save the resized images back to .npy files\n",
        "np.save('train_images_resized.npy', train_images_resized)\n",
        "np.save('test_images_resized.npy', test_images_resized)\n",
        "\n",
        "# Optionally save the resized images to disk as image files\n",
        "save_images(train_images_resized, train_labels, 'resized_train_images', 'train')\n",
        "save_images(test_images_resized, test_labels, 'resized_test_images', 'test')\n",
        "\n",
        "print(\"Resizing completed and files saved.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XC7t7ufO_BPP",
        "outputId": "e4cad9c1-7dab-4cfb-e1df-414a696ce074"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resizing completed and files saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Load resized test images and labels\n",
        "test_images_resized = np.load('test_images_resized.npy')\n",
        "test_labels = np.load('test_labels_one_hot.npy')\n"
      ],
      "metadata": {
        "id": "ntBRdI0NZ-Es"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "\n",
        "# Example model architecture (replace with my actual model architecture)\n",
        "model = Sequential([\n",
        "    Flatten(input_shape=(32, 32, 3)),  # Assuming resized images are 32x32 pixels with 3 channels\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(10, activation='softmax')   # Assuming 10 classes for classification\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model (example, replace with my actual training code)\n",
        "model.fit(train_images_resized, train_labels_one_hot, epochs=10, batch_size=32, validation_data=(test_images_resized, test_labels))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9rua2YH8aTuQ",
        "outputId": "3a7a78b3-9cee-4a2e-bcc3-920880dc63cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1563/1563 [==============================] - 17s 10ms/step - loss: 1.9143 - accuracy: 0.3133 - val_loss: 1.7947 - val_accuracy: 0.3529\n",
            "Epoch 2/10\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 1.7704 - accuracy: 0.3651 - val_loss: 1.7357 - val_accuracy: 0.3803\n",
            "Epoch 3/10\n",
            "1563/1563 [==============================] - 11s 7ms/step - loss: 1.7108 - accuracy: 0.3868 - val_loss: 1.7400 - val_accuracy: 0.3736\n",
            "Epoch 4/10\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 1.6828 - accuracy: 0.3970 - val_loss: 1.6637 - val_accuracy: 0.4054\n",
            "Epoch 5/10\n",
            "1563/1563 [==============================] - 11s 7ms/step - loss: 1.6602 - accuracy: 0.4089 - val_loss: 1.7043 - val_accuracy: 0.3896\n",
            "Epoch 6/10\n",
            "1563/1563 [==============================] - 16s 11ms/step - loss: 1.6430 - accuracy: 0.4133 - val_loss: 1.6432 - val_accuracy: 0.4104\n",
            "Epoch 7/10\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 1.6306 - accuracy: 0.4159 - val_loss: 1.6206 - val_accuracy: 0.4202\n",
            "Epoch 8/10\n",
            "1563/1563 [==============================] - 12s 7ms/step - loss: 1.6135 - accuracy: 0.4224 - val_loss: 1.6028 - val_accuracy: 0.4264\n",
            "Epoch 9/10\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 1.6120 - accuracy: 0.4209 - val_loss: 1.6271 - val_accuracy: 0.4154\n",
            "Epoch 10/10\n",
            "1563/1563 [==============================] - 11s 7ms/step - loss: 1.6010 - accuracy: 0.4299 - val_loss: 1.6125 - val_accuracy: 0.4189\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7a4dea9a3a60>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Assuming my model is already trained, make predictions\n",
        "predicted_labels = model.predict(test_images_resized)\n",
        "\n",
        "# Convert one-hot encoded labels back to categorical labels if needed\n",
        "true_labels = np.argmax(test_labels, axis=1)\n",
        "predicted_labels = np.argmax(predicted_labels, axis=1)\n",
        "\n",
        "# Compute evaluation metrics\n",
        "accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "precision = precision_score(true_labels, predicted_labels, average='weighted')\n",
        "recall = recall_score(true_labels, predicted_labels, average='weighted')\n",
        "f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
        "\n",
        "# Print or use these metrics as needed\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1 Score: {f1}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jqrpvmecg5y",
        "outputId": "ec5c14a5-f378-4136-b8b1-558b1808fbf7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 4s 10ms/step\n",
            "Accuracy: 0.4189\n",
            "Precision: 0.40989567381337483\n",
            "Recall: 0.4189\n",
            "F1 Score: 0.39758468165670474\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Load and Prepare Data\n",
        "\n",
        "First, ensure I load resized test images (test_images_resized.npy) and their corresponding labels (test_labels_one_hot.npy):\n",
        "\n",
        "\n",
        "2. Train Machine Learning Model\n",
        "\n",
        "Before make predictions, i need to have a trained model. Here’s a hypothetical example assuming using TensorFlow/Keras for training:\n",
        "\n",
        "3. Make Predictions and Evaluate\n",
        "\n",
        "After training the model, then proceed to make predictions on the test data (test_images_resized) and compute evaluation metrics:\n",
        "\n",
        "Explanation:\n",
        "Model Definition and Training: I define a simple neural network model using TensorFlow/Keras, compile it with an optimizer and loss function, and then train it on my resized training data (train_images_resized and train_labels_one_hot).\n",
        "\n",
        "Prediction: After training, the model.predict method is used to generate predictions on my resized test images (test_images_resized).\n",
        "\n",
        "Evaluation Metrics: Using scikit-learn's metrics (accuracy_score, precision_score, recall_score, f1_score), i compute evaluation metrics comparing the predicted labels (predicted_labels) with the true labels (true_labels).\n",
        "\n",
        "Ensure that I replace placeholders (train_images_resized, train_labels_one_hot, test_images_resized, test_labels_one_hot) with my actual data variables as per my dataset structure. Additionally, adjust the model architecture and training parameters to suit my specific machine learning task and dataset characteristics. This structured approach will help me effectively integrate model training, prediction, and evaluation in my machine learning workflow."
      ],
      "metadata": {
        "id": "CF6-r47tcySQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation:\n",
        "Import Libraries:\n",
        "\n",
        "numpy for handling array operations.\n",
        "cv2 (OpenCV) for resizing images.\n",
        "Define a Function to Resize Images:\n",
        "\n",
        "resize_images function takes an array of images and a new size (width, height) as inputs and returns the resized images.\n",
        "Define a Function to Save Images (optional):\n",
        "\n",
        "save_images function saves the resized images as actual image files in a specified directory, along with their labels.\n",
        "Load .npy Files:\n",
        "\n",
        "Load my .npy files containing the image data using np.load.\n",
        "Define the New Size:\n",
        "\n",
        "Set the target size for the images (e.g., 32x32 pixels).\n",
        "Resize the Images:\n",
        "\n",
        "Use the cv2.resize function to resize each image to the new size.\n",
        "Save the Resized Images:\n",
        "\n",
        "Save the resized image arrays back to .npy files using np.save.\n",
        "Optional: Save Images as Files:\n",
        "\n",
        "If i want to save the resized images as actual image files, use the save_images function.\n",
        "\n",
        "Notes:\n",
        "Ensure the dimensions of the input images are compatible with the cv2.resize function. If the images are in grayscale or have a different number of channels,i might need to adjust the script accordingly.\n",
        "\n",
        "The cv2.INTER_AREA interpolation method is generally good for shrinking images. i can experiment with other interpolation methods provided by OpenCV (e.g., cv2.INTER_LINEAR, cv2.INTER_CUBIC) to see which one works best for my specific use case.\n",
        "\n",
        "The optional step of saving images as files is useful if i need to visually inspect the resized images or use them in a different format.\n",
        "\n",
        "By following these steps, i can resize image datasets stored in .npy files and prepare them for further processing or model training.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bauKP4KK_1-Z"
      }
    }
  ]
}